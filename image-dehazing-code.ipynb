{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f25ab3a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:35.771105Z",
     "iopub.status.busy": "2024-01-15T18:26:35.770229Z",
     "iopub.status.idle": "2024-01-15T18:26:39.394314Z",
     "shell.execute_reply": "2024-01-15T18:26:39.393317Z"
    },
    "papermill": {
     "duration": 3.633691,
     "end_time": "2024-01-15T18:26:39.396662",
     "exception": false,
     "start_time": "2024-01-15T18:26:35.762971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time, math\n",
    "import argparse, random\n",
    "from math import exp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as tfs\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import functional as FF\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d031e8e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:39.410004Z",
     "iopub.status.busy": "2024-01-15T18:26:39.409214Z",
     "iopub.status.idle": "2024-01-15T18:26:39.467926Z",
     "shell.execute_reply": "2024-01-15T18:26:39.467145Z"
    },
    "papermill": {
     "duration": 0.067187,
     "end_time": "2024-01-15T18:26:39.469909",
     "exception": false,
     "start_time": "2024-01-15T18:26:39.402722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# number of training steps\n",
    "steps = 20000\n",
    "# Device name\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# resume Training\n",
    "resume = False\n",
    "# number of evaluation steps\n",
    "eval_step = 500\n",
    "# learning rate\n",
    "learning_rate = 0.0001\n",
    "# pre-trained model directory\n",
    "pretrained_model_dir = './trained_models/'\n",
    "# directory to save models to\n",
    "model_dir = './trained_models/'\n",
    "# train data\n",
    "trainset = 'its_train'\n",
    "# test data\n",
    "testset = 'its_test'\n",
    "# model to be used\n",
    "network = 'ffa'\n",
    "# residual_groups\n",
    "gps = 3\n",
    "# residual_blocks\n",
    "blocks = 12\n",
    "# batch size\n",
    "bs = 1\n",
    "# crop image\n",
    "crop = True\n",
    "# Takes effect when crop = True\n",
    "crop_size = 240\n",
    "# No lr cos schedule\n",
    "no_lr_sche = True\n",
    "# perceptual loss\n",
    "perloss = True\n",
    "\n",
    "model_name = trainset + '_' + network.split('.')[0] + '_' + str(gps) + '_' + str(blocks)\n",
    "pretrained_model_dir = pretrained_model_dir + model_name + '.pk'\n",
    "model_dir = model_dir + model_name + '.pk'\n",
    "log_dir = 'logs/' + model_name\n",
    "\n",
    "if not os.path.exists('trained_models'):\n",
    "    os.mkdir('trained_models')\n",
    "if not os.path.exists('numpy_files'):\n",
    "    os.mkdir('numpy_files')\n",
    "if not os.path.exists('logs'):\n",
    "    os.mkdir('logs')\n",
    "if not os.path.exists('samples'):\n",
    "    os.mkdir('samples')\n",
    "if not os.path.exists(f\"samples/{model_name}\"):\n",
    "    os.mkdir(f'samples/{model_name}')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "    \n",
    "crop_size='whole_img'\n",
    "if crop:\n",
    "    crop_size = crop_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69341ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:39.482127Z",
     "iopub.status.busy": "2024-01-15T18:26:39.481802Z",
     "iopub.status.idle": "2024-01-15T18:26:54.384567Z",
     "shell.execute_reply": "2024-01-15T18:26:54.383537Z"
    },
    "papermill": {
     "duration": 14.911959,
     "end_time": "2024-01-15T18:26:54.387102",
     "exception": false,
     "start_time": "2024-01-15T18:26:39.475143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\r\n",
      "  Obtaining dependency information for einops from https://files.pythonhosted.org/packages/29/0b/2d1c0ebfd092e25935b86509a9a817159212d82aa43d7fb07eca4eeff2c2/einops-0.7.0-py3-none-any.whl.metadata\r\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: einops\r\n",
      "Successfully installed einops-0.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "from functools import partial\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def cast_tuple(val, length = 1):\n",
    "    return val if isinstance(val, tuple) else ((val,) * length)\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class ChanLayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
    "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(dim_in, dim_out, 3, stride = 2, padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class PEG(nn.Module):\n",
    "    def __init__(self, dim, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim, kernel_size = kernel_size, padding = kernel_size // 2, groups = dim, stride = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x) + x\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, expansion_factor = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            ChanLayerNorm(dim),\n",
    "            nn.Conv2d(dim, inner_dim, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(inner_dim, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# attention\n",
    "\n",
    "class ScalableSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_key = 32,\n",
    "        dim_value = 32,\n",
    "        dropout = 0.,\n",
    "        reduction_factor = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_key ** -0.5\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm = ChanLayerNorm(dim)\n",
    "        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)\n",
    "        self.to_k = nn.Conv2d(dim, dim_key * heads, reduction_factor, stride = reduction_factor, bias = False)\n",
    "        self.to_v = nn.Conv2d(dim, dim_value * heads, reduction_factor, stride = reduction_factor, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(dim_value * heads, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        height, width, heads = *x.shape[-2:], self.heads\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n",
    "\n",
    "        # split out heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h d) ... -> b h (...) d', h = heads), (q, k, v))\n",
    "\n",
    "        # similarity\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # merge back heads\n",
    "\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = height, y = width)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class InteractiveWindowedSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        heads = 8,\n",
    "        dim_key = 32,\n",
    "        dim_value = 32,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_key ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm = ChanLayerNorm(dim)\n",
    "        self.local_interactive_module = nn.Conv2d(dim_value * heads, dim_value * heads, 3, padding = 1)\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)\n",
    "        self.to_k = nn.Conv2d(dim, dim_key * heads, 1, bias = False)\n",
    "        self.to_v = nn.Conv2d(dim, dim_value * heads, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(dim_value * heads, dim, 1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        height, width, heads, wsz = *x.shape[-2:], self.heads, self.window_size\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        wsz_h, wsz_w = default(wsz, height), default(wsz, width)\n",
    "        assert (height % wsz_h) == 0 and (width % wsz_w) == 0, f'height ({height}) or width ({width}) of feature map is not divisible by the window size ({wsz_h}, {wsz_w})'\n",
    "\n",
    "        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)\n",
    "\n",
    "        # get output of LIM\n",
    "\n",
    "        local_out = self.local_interactive_module(v)\n",
    "\n",
    "        # divide into window (and split out heads) for efficient self attention\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h d) (x w1) (y w2) -> (b x y) h (w1 w2) d', h = heads, w1 = wsz_h, w2 = wsz_w), (q, k, v))\n",
    "\n",
    "        # similarity\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # reshape the windows back to full feature map (and merge heads)\n",
    "\n",
    "        out = rearrange(out, '(b x y) h (w1 w2) d -> b (h d) (x w1) (y w2)', x = height // wsz_h, y = width // wsz_w, w1 = wsz_h, w2 = wsz_w)\n",
    "\n",
    "        # add LIM output \n",
    "\n",
    "        out = out + local_out\n",
    "\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 8,\n",
    "        ff_expansion_factor = 4,\n",
    "        dropout = 0.,\n",
    "        ssa_dim_key = 32,\n",
    "        ssa_dim_value = 32,\n",
    "        ssa_reduction_factor = 1,\n",
    "        iwsa_dim_key = 32,\n",
    "        iwsa_dim_value = 32,\n",
    "        iwsa_window_size = None,\n",
    "        norm_output = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for ind in range(depth):\n",
    "            is_first = ind == 0\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                ScalableSelfAttention(dim, heads = heads, dim_key = ssa_dim_key, dim_value = ssa_dim_value, reduction_factor = ssa_reduction_factor, dropout = dropout),\n",
    "                FeedForward(dim, expansion_factor = ff_expansion_factor, dropout = dropout),\n",
    "                PEG(dim) if is_first else None,\n",
    "                FeedForward(dim, expansion_factor = ff_expansion_factor, dropout = dropout),\n",
    "                InteractiveWindowedSelfAttention(dim, heads = heads, dim_key = iwsa_dim_key, dim_value = iwsa_dim_value, window_size = iwsa_window_size, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "        self.norm = ChanLayerNorm(dim) if norm_output else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for ssa, ff1, peg, iwsa, ff2 in self.layers:\n",
    "            x = ssa(x) + x\n",
    "            x = ff1(x) + x\n",
    "\n",
    "            if exists(peg):\n",
    "                x = peg(x)\n",
    "\n",
    "            x = iwsa(x) + x\n",
    "            x = ff2(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ScalableViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        reduction_factor,\n",
    "        window_size = None,\n",
    "        iwsa_dim_key = 32,\n",
    "        iwsa_dim_value = 32,\n",
    "        ssa_dim_key = 32,\n",
    "        ssa_dim_value = 32,\n",
    "        ff_expansion_factor = 4,\n",
    "        channels = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.to_patches = nn.Conv2d(channels, dim, 7, stride = 4, padding = 3)\n",
    "\n",
    "        assert isinstance(depth, tuple), 'depth needs to be tuple if integers indicating number of transformer blocks at that stage'\n",
    "\n",
    "        num_stages = len(depth)\n",
    "        dims = tuple(map(lambda i: (2 ** i) * dim, range(num_stages)))\n",
    "\n",
    "        hyperparams_per_stage = [\n",
    "            heads,\n",
    "            ssa_dim_key,\n",
    "            ssa_dim_value,\n",
    "            reduction_factor,\n",
    "            iwsa_dim_key,\n",
    "            iwsa_dim_value,\n",
    "            window_size,\n",
    "        ]\n",
    "\n",
    "        hyperparams_per_stage = list(map(partial(cast_tuple, length = num_stages), hyperparams_per_stage))\n",
    "        assert all(tuple(map(lambda arr: len(arr) == num_stages, hyperparams_per_stage)))\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for ind, (layer_dim, layer_depth, layer_heads, layer_ssa_dim_key, layer_ssa_dim_value, layer_ssa_reduction_factor, layer_iwsa_dim_key, layer_iwsa_dim_value, layer_window_size) in enumerate(zip(dims, depth, *hyperparams_per_stage)):\n",
    "            is_last = ind == (num_stages - 1)\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(dim = layer_dim, depth = layer_depth, heads = layer_heads, ff_expansion_factor = ff_expansion_factor, dropout = dropout, ssa_dim_key = layer_ssa_dim_key, ssa_dim_value = layer_ssa_dim_value, ssa_reduction_factor = layer_ssa_reduction_factor, iwsa_dim_key = layer_iwsa_dim_key, iwsa_dim_value = layer_iwsa_dim_value, iwsa_window_size = layer_window_size, norm_output = not is_last),\n",
    "                Downsample(layer_dim, layer_dim * 2) if not is_last else None\n",
    "            ]))\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            Reduce('b d h w -> b d', 'mean'),\n",
    "            nn.LayerNorm(dims[-1]),\n",
    "            nn.Linear(dims[-1], num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patches(img)\n",
    "        #print(x.shape,self.layers)\n",
    "        x_fuses = []\n",
    "\n",
    "        for transformer, downsample in self.layers:\n",
    "            x = transformer(x)\n",
    "            x_fuses.append(x)\n",
    "            #print('bb ',x.shape)\n",
    "\n",
    "            if exists(downsample):\n",
    "                x = downsample(x)\n",
    "                #print('xx ',x.shape)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        return x_fuses#self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30929eb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:54.400711Z",
     "iopub.status.busy": "2024-01-15T18:26:54.400219Z",
     "iopub.status.idle": "2024-01-15T18:26:54.455106Z",
     "shell.execute_reply": "2024-01-15T18:26:54.454180Z"
    },
    "papermill": {
     "duration": 0.064147,
     "end_time": "2024-01-15T18:26:54.457256",
     "exception": false,
     "start_time": "2024-01-15T18:26:54.393109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "# from scalable_vit import ScalableViT\n",
    "###############################################################################\n",
    "# Helper Functions\n",
    "###############################################################################\n",
    "def get_norm_layer(norm_type='instance'):\n",
    "\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n",
    "    elif norm_type == 'none':\n",
    "        norm_layer = None\n",
    "    else:\n",
    "        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, opt):\n",
    "\n",
    "    if opt.lr_policy == 'linear':\n",
    "        def lambda_rule(epoch):\n",
    "            lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n",
    "            return lr_l\n",
    "        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "    elif opt.lr_policy == 'step':\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n",
    "    elif opt.lr_policy == 'plateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n",
    "    elif opt.lr_policy == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n",
    "    else:\n",
    "        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size//2), bias=bias)\n",
    "\n",
    "def conv_layers(inp, oup, dilation):\n",
    "    #if dilation:\n",
    "    d_rate = dilation\n",
    "    #else:\n",
    "    #    d_rate = 1\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernel_size=3, padding=d_rate, dilation=d_rate),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def feature_transform(inp, oup):\n",
    "    conv2d = nn.Conv2d(inp, oup, kernel_size=1)  # no padding\n",
    "    relu = nn.ReLU(inplace=True)\n",
    "    layers = []\n",
    "    layers += [conv2d, relu]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def pool_layers(ceil_mode=True):\n",
    "    return nn.MaxPool2d(kernel_size=5, stride=2)\n",
    "\n",
    "class block_V(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact Dilation Convolution based Module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(block_V, self).__init__()\n",
    "        \n",
    "\n",
    "        self.conv0_0 = conv_layers(in_channels, in_channels,1)\n",
    "\n",
    "\n",
    "        self.pool0 = pool_layers()\n",
    "        self.conv1_0 = conv_layers(in_channels, in_channels,2)\n",
    "        self.conv1_1 = conv_layers(in_channels, in_channels,2)\n",
    "\n",
    "        self.pool1 = pool_layers()\n",
    "        self.conv2_0 = conv_layers(in_channels, in_channels,4)\n",
    "        self.conv2_1 = conv_layers(in_channels, in_channels,4)\n",
    "        \n",
    "        self.pool2 = pool_layers()\n",
    "        self.conv3_0 = conv_layers(in_channels, in_channels,8)\n",
    "        self.conv3_1 = conv_layers(in_channels, in_channels,8)\n",
    "\n",
    "        \n",
    "      \n",
    "        \n",
    "    def forward(self, x):\n",
    "        H, W = x.size()[2:]\n",
    "\n",
    "        x = self.conv0_0(x)\n",
    "\n",
    "        x = self.pool0(x)\n",
    "        x = self.conv1_0(x)      \n",
    "        x1 = self.conv1_1(x)\n",
    "       \n",
    "        x = self.pool1(x1)      \n",
    "        x = self.conv2_0(x)\n",
    "        x2 = self.conv2_1(x)\n",
    "             \n",
    "        x = self.pool2(x2)\n",
    "        x = self.conv3_0(x)       \n",
    "        x3 = self.conv3_1(x)\n",
    "\n",
    "        return [x1,x2,x3]\n",
    "\n",
    "class block_O(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact Dilation Convolution based Module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(block_O, self).__init__()\n",
    "\n",
    "        self.conv1_0 = conv_layers(in_channels, in_channels,2)\n",
    "        self.conv1_1 = conv_layers(in_channels, in_channels,2)\n",
    "   \n",
    "        self.conv2_0 = conv_layers(in_channels, in_channels,2)\n",
    "        self.conv2_1 = conv_layers(in_channels, in_channels,2)\n",
    "              \n",
    "        self.conv3_0 = conv_layers(in_channels, in_channels,2)\n",
    "        self.conv3_1 = conv_layers(in_channels, in_channels,2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1_0(x)      \n",
    "        x1 = self.conv1_1(x)\n",
    "             \n",
    "        x = self.conv2_0(x1)\n",
    "        x2 = self.conv2_1(x)\n",
    "             \n",
    "        x = self.conv3_0(x2)       \n",
    "        x3 = self.conv3_1(x)\n",
    "        \n",
    "        return [x1,x2,x3]\n",
    "        \n",
    "class blockbn(nn.Module):\n",
    "    \"\"\"\n",
    "    Compact Dilation Convolution based Module\n",
    "    \"\"\"\n",
    "    def __init__(self, inn):\n",
    "        super(blockbn, self).__init__()\n",
    "        #self.norm_layer = get_norm_layer(norm_type='batch')\n",
    "        norm_layer = get_norm_layer(norm_type='batch')\n",
    "        \n",
    "\n",
    "        self.conv0_0= ScalableViT(\n",
    "            num_classes = 1000,\n",
    "            dim = inn,                               # starting model dimension. at every stage, dimension is doubled\n",
    "            heads = (2, 4, 8, 16),                  # number of attention heads at each stage\n",
    "            depth = (2, 2, 20, 2),                  # number of transformer blocks at each stage\n",
    "            ssa_dim_key = (4, 4, 4, 4),         # the dimension of the attention keys (and queries) for SSA. in the paper, they represented this as a scale factor on the base dimension per key (ssa_dim_key / dim_key)\n",
    "            reduction_factor = (8, 4, 2, 1),        # downsampling of the key / values in SSA. in the paper, this was represented as (reduction_factor ** -2)\n",
    "            window_size = (16, 16, None, None),     # window size of the IWSA at each stage. None means no windowing needed\n",
    "            dropout = 0.1,                          # attention and feedforward dropout\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        #self.pool1 = pool_layers()\n",
    "        self.c0 = nn.Conv2d(32, inn, 3, padding=1)\n",
    "        self.c1 = nn.Conv2d(64, inn, 3, padding=1)\n",
    "        self.c2 = nn.Conv2d(128, inn, 3, padding=1)\n",
    "        self.c3 = nn.Conv2d(256, inn, 3, padding=1)\n",
    "        self.c4 = nn.Conv2d(512, inn, 3, padding=1)\n",
    "\n",
    "        #self.conv2_0 = ScalableSelfAttention(inn, heads = 2, dim_key = 40, dim_value = 32, reduction_factor = 8, dropout = 0.1)\n",
    "  \n",
    "       \n",
    "        self.classifier = nn.Conv2d(inn*3, 3, kernel_size=1)\n",
    "    def _conv_block(self, in_nc, out_nc, norm_layer, num_block=1, kernel_size=3,stride=1, padding=2,bias=False):\n",
    "        conv = []\n",
    "        for i in range(num_block):\n",
    "            cur_in_nc = in_nc if i == 0 else out_nc\n",
    "            conv += [nn.Conv2d(cur_in_nc, out_nc, kernel_size=kernel_size, stride=stride, \n",
    "                               padding=padding, dilation=padding, bias=False),\n",
    "                     norm_layer(out_nc),\n",
    "                     nn.ReLU(True)]\n",
    "        return conv\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "        #print(x.shape)\n",
    "        H, W = x.size()[2:]\n",
    "        if x.size()[1] == 128:\n",
    "            \n",
    "            x = self.cc2(x)\n",
    "            \n",
    "        \n",
    "        [x1,x2,x3,x4] = self.conv0_0(x)\n",
    "       \n",
    "        x1 = F.interpolate(x1, (H, W), mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "\n",
    "        \n",
    "        x2 = F.interpolate(x2, (H, W), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "        x3 = F.interpolate(x3, (H, W), mode=\"bilinear\", align_corners=False)\n",
    "        x4 = F.interpolate(x4, (H, W), mode=\"bilinear\", align_corners=False)\n",
    "        #x3=x1+x2\n",
    "        x1=self.c1(x1)\n",
    "        x2=self.c2(x2)\n",
    "        x3=self.c3(x3)\n",
    "        x4=self.c4(x4)\n",
    "        #print(x1.shape,x2.shape,x3.shape,x4.shape,)\n",
    "\n",
    "        return [x1,x2,x3,x4]\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    \"\"\"Return an activation function given a string\"\"\"\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    if activation == \"gelu\":\n",
    "        return F.gelu\n",
    "    if activation == \"glu\":\n",
    "        return F.glu\n",
    "    raise RuntimeError(F\"activation should be relu/gelu, not {activation}.\")\n",
    "\n",
    "\n",
    "class FFA(nn.Module):\n",
    "    def __init__(self,gps = gps, blocks = blocks):\n",
    "        super(FFA, self).__init__()\n",
    "\n",
    "        self.bn=blockbn(64)\n",
    "        self.bn1=blockbn(128)\n",
    "        self.bn2=blockbn(128)\n",
    "        self.conv1_1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv1_3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        self.conv2_1 = nn.Conv2d(64,128, 3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        \n",
    "        #self.conv4_1 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        #self.conv4_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "        self.convo1 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.convo2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.convo3 = nn.Conv2d(32, 3, 3, padding=1)\n",
    "        self.tt = nn.Conv2d(128, 32, 3, padding=1)\n",
    "       \n",
    "        self.p = nn.MaxPool2d(3, stride=2)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.score_final = nn.Conv2d(12, 3, 1)\n",
    "        \n",
    "        nn.init.constant_(self.score_final.weight, 0.25)\n",
    "        nn.init.constant_(self.score_final.bias, 0)\n",
    "\n",
    "        print('initialization done')\n",
    "        \n",
    "    def _conv_block(self, in_nc, out_nc, norm_layer, num_block=1, kernel_size=1,stride=1,bias=False):\n",
    "        conv = []\n",
    "        for i in range(num_block):\n",
    "            cur_in_nc = in_nc if i == 0 else out_nc\n",
    "            conv += [nn.Conv2d(cur_in_nc, out_nc, kernel_size=kernel_size, stride=stride, bias=False),\n",
    "                     norm_layer(out_nc),\n",
    "                     nn.ReLU(True)]\n",
    "        return conv\n",
    "    def get_weights(self):\n",
    "        conv_weights = []\n",
    "        bn_weights = []\n",
    "        relu_weights = []\n",
    "        for pname, p in self.named_parameters():\n",
    "            if 'bn' in pname:\n",
    "                bn_weights.append(p)\n",
    "            elif 'relu' in pname:\n",
    "                relu_weights.append(p)\n",
    "            else:\n",
    "                conv_weights.append(p)\n",
    "\n",
    "        return conv_weights, bn_weights, relu_weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        H1, W1 = x.size()[2:]\n",
    "        conv1 = self.conv1_1(x)\n",
    "        conv2 = self.conv1_2(conv1)\n",
    "        \n",
    "        [z1,z2,z3,z4] = self.bn(conv2)\n",
    "        y1= self.conv2_1(z1+conv2)\n",
    "        y2= self.conv2_1(z1+z2)\n",
    "        y3= self.conv2_1(z2+z3)\n",
    "        y4= self.conv2_1(z1+z2+z3+z4)\n",
    "        \n",
    "        t1=self.conv2_2(y1+y2)\n",
    "        t2=self.conv2_2(y2+y3)\n",
    "        t3=self.conv2_2(y3+y4)\n",
    "\n",
    "        #print(y1.shape,y2.shape,y3.shape,y4.shape,t1.shape,t2.shape,t3.shape,(t1+t2+t3).shape)\n",
    "        #orig = self.conv1_3(conv2)\n",
    "        fuse0=self.conv3_1(t1+t2+t3)\n",
    "        fuse1=self.conv3_2(fuse0)\n",
    "        fuse2 = self.convo1(fuse1)#fuse1)\n",
    "        fuse2 = self.convo2(fuse2)#fuse1)\n",
    "        #print(fuse2.shape)\n",
    "        fuse3 = self.convo3(fuse2)\n",
    "        fuse = F.interpolate(fuse3, (H1, W1), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return fuse#results\n",
    "\n",
    "# model = FFA()\n",
    "# x = torch.randn(1, 3, 256, 256)\n",
    "# print(model(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14570030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:54.470255Z",
     "iopub.status.busy": "2024-01-15T18:26:54.469524Z",
     "iopub.status.idle": "2024-01-15T18:26:54.477497Z",
     "shell.execute_reply": "2024-01-15T18:26:54.476595Z"
    },
    "papermill": {
     "duration": 0.016305,
     "end_time": "2024-01-15T18:26:54.479329",
     "exception": false,
     "start_time": "2024-01-15T18:26:54.463024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Perceptual loss network  --- #\n",
    "class PerLoss(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        super(PerLoss, self).__init__()\n",
    "        self.vgg_layers = vgg_model\n",
    "        self.layer_name_mapping = {\n",
    "            '3': \"relu1_2\",\n",
    "            '8': \"relu2_2\",\n",
    "            '15': \"relu3_3\"\n",
    "        }\n",
    "\n",
    "    def output_features(self, x):\n",
    "        output = {}\n",
    "        for name, module in self.vgg_layers._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layer_name_mapping:\n",
    "                output[self.layer_name_mapping[name]] = x\n",
    "        return list(output.values())\n",
    "\n",
    "    def forward(self, dehaze, gt):\n",
    "        loss = []\n",
    "        dehaze_features = self.output_features(dehaze)\n",
    "        gt_features = self.output_features(gt)\n",
    "        for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
    "            loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
    "\n",
    "        return sum(loss)/len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6a9742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:54.493278Z",
     "iopub.status.busy": "2024-01-15T18:26:54.492939Z",
     "iopub.status.idle": "2024-01-15T18:26:54.507235Z",
     "shell.execute_reply": "2024-01-15T18:26:54.506387Z"
    },
    "papermill": {
     "duration": 0.024255,
     "end_time": "2024-01-15T18:26:54.509261",
     "exception": false,
     "start_time": "2024-01-15T18:26:54.485006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    img1=torch.clamp(img1,min=0,max=1)\n",
    "    img2=torch.clamp(img2,min=0,max=1)\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
    "\n",
    "def psnr(pred, gt):\n",
    "    pred=pred.clamp(0,1).cpu().numpy()\n",
    "    gt=gt.clamp(0,1).cpu().numpy()\n",
    "    imdff = pred - gt\n",
    "    rmse = math.sqrt(np.mean(imdff ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    return 20 * math.log10( 1.0 / rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af42580f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:54.521709Z",
     "iopub.status.busy": "2024-01-15T18:26:54.521405Z",
     "iopub.status.idle": "2024-01-15T18:26:54.584548Z",
     "shell.execute_reply": "2024-01-15T18:26:54.583610Z"
    },
    "papermill": {
     "duration": 0.071845,
     "end_time": "2024-01-15T18:26:54.586718",
     "exception": false,
     "start_time": "2024-01-15T18:26:54.514873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RESIDE_Dataset(data.Dataset):\n",
    "    def __init__(self, path, train, size=crop_size, format='.png'):\n",
    "        super(RESIDE_Dataset, self).__init__()\n",
    "        self.size = size\n",
    "        self.train = train\n",
    "        self.format = format\n",
    "        self.haze_imgs_dir = os.listdir(os.path.join(path,'hazy'))\n",
    "        self.haze_imgs = [os.path.join(path, 'hazy', img) for img in self.haze_imgs_dir]\n",
    "        self.clear_dir = os.path.join(path,'clear')\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        haze = Image.open(self.haze_imgs[index])\n",
    "        haze=haze.resize((256, 256), Image.BICUBIC)\n",
    "        if isinstance(self.size, int):\n",
    "            while haze.size[0] < self.size or haze.size[1] < self.size :\n",
    "                index = random.randint(0, 2000)\n",
    "                haze = Image.open(self.haze_imgs[index])\n",
    "        img = self.haze_imgs[index]\n",
    "        id = img.split('/')[-1]#.split('_')[0]\n",
    "        clear_name = id #+ self.format\n",
    "        clear_name=clear_name.split('\\\\')[-1]\n",
    "    \n",
    "        \n",
    "        clear = Image.open(os.path.join(self.clear_dir, clear_name))\n",
    "        clear =clear.resize((256, 256), Image.BICUBIC)\n",
    "        #print(self.haze_imgs[index],os.path.join(self.clear_dir, clear_name))\n",
    "        clear = tfs.CenterCrop(haze.size[::-1])(clear)\n",
    "        if not isinstance(self.size, str):\n",
    "            i, j, h, w = tfs.RandomCrop.get_params(haze, output_size=(self.size, self.size))\n",
    "            haze = FF.crop(haze, i, j, h, w)\n",
    "            clear = FF.crop(clear, i, j, h, w)\n",
    "        haze, clear = self.augData(haze.convert(\"RGB\"), clear.convert(\"RGB\") )\n",
    "        return haze, clear\n",
    "    \n",
    "    def augData(self, data, target):\n",
    "        if self.train:\n",
    "            rand_hor = random.randint(0,1)\n",
    "            rand_rot = random.randint(0,3)\n",
    "            data = tfs.RandomHorizontalFlip(rand_hor)(data)\n",
    "            target = tfs.RandomHorizontalFlip(rand_hor)(target)\n",
    "            if rand_rot:\n",
    "                data = FF.rotate(data, 90*rand_rot)\n",
    "                target = FF.rotate(target, 90*rand_rot)\n",
    "        data = tfs.ToTensor()(data)\n",
    "        data = tfs.Normalize(mean=[0.64,0.6,0.58], std=[0.14,0.15,0.152])(data)\n",
    "        target = tfs.ToTensor()(target)\n",
    "        return data, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.haze_imgs)\n",
    "\n",
    "\n",
    "# path to your 'data' folder\n",
    "its_train_path = '/kaggle/input/o-haze'\n",
    "its_test_path = '/kaggle/input/o-haze'\n",
    "\n",
    "ITS_train_loader = DataLoader(dataset=RESIDE_Dataset(its_train_path, train=True, size=crop_size), batch_size=bs, shuffle=True)\n",
    "ITS_test_loader = DataLoader(dataset=RESIDE_Dataset(its_test_path, train=False, size='whole img'), batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49b7a4c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:54.600691Z",
     "iopub.status.busy": "2024-01-15T18:26:54.600321Z",
     "iopub.status.idle": "2024-01-15T18:26:58.482059Z",
     "shell.execute_reply": "2024-01-15T18:26:58.481069Z"
    },
    "papermill": {
     "duration": 3.891157,
     "end_time": "2024-01-15T18:26:58.484132",
     "exception": false,
     "start_time": "2024-01-15T18:26:54.592975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir : logs/its_train_ffa_3_12\n",
      "model_name: its_train_ffa_3_12\n",
      "initialization done\n"
     ]
    }
   ],
   "source": [
    "print('log_dir :', log_dir)\n",
    "print('model_name:', model_name)\n",
    "\n",
    "models_ = {'ffa': FFA(gps = gps, blocks = blocks)}\n",
    "loaders_ = {'its_train': ITS_train_loader, 'its_test': ITS_test_loader}\n",
    "# loaders_ = {'its_train': ITS_train_loader, 'its_test': ITS_test_loader, 'ots_train': OTS_train_loader, 'ots_test': OTS_test_loader}\n",
    "start_time = time.time()\n",
    "T = steps\n",
    "\n",
    "def train(net, loader_train, loader_test, optim, criterion):\n",
    "    losses = []\n",
    "    start_step = 0\n",
    "    max_ssim = max_psnr = 0\n",
    "    ssims, psnrs = [], []\n",
    "    if resume and os.path.exists(pretrained_model_dir):\n",
    "        print(f'resume from {pretrained_model_dir}')\n",
    "        ckp = torch.load(pretrained_model_dir)\n",
    "        losses = ckp['losses']\n",
    "        net.load_state_dict(ckp['model'])\n",
    "        start_step = ckp['step']\n",
    "        max_ssim = ckp['max_ssim']\n",
    "        max_psnr = ckp['max_psnr']\n",
    "        psnrs = ckp['psnrs']\n",
    "        ssims = ckp['ssims']\n",
    "        print(f'Resuming training from step: {start_step} ***')\n",
    "    else :\n",
    "        print('Training from scratch *** ')\n",
    "    for step in range(start_step+1, steps+1):\n",
    "        net.train()\n",
    "        lr = learning_rate\n",
    "        if not no_lr_sche:\n",
    "            lr = lr_schedule_cosdecay(step,T)\n",
    "            for param_group in optim.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "        x, y = next(iter(loader_train))\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        out = net(x)\n",
    "        loss = criterion[0](out,y)\n",
    "        if perloss:\n",
    "            loss2 = criterion[1](out,y)\n",
    "            loss = loss + 0.04*loss2\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        print(f'\\rtrain loss: {loss.item():.5f} | step: {step}/{steps} | lr: {lr :.7f} | time_used: {(time.time()-start_time)/60 :.1f}',end='',flush=True)\n",
    "\n",
    "        if step % eval_step ==0 :\n",
    "            with torch.no_grad():\n",
    "                ssim_eval, psnr_eval = test(net, loader_test, max_psnr, max_ssim, step)\n",
    "            print(f'\\nstep: {step} | ssim: {ssim_eval:.4f} | psnr: {psnr_eval:.4f}')\n",
    "\n",
    "            ssims.append(ssim_eval)\n",
    "            psnrs.append(psnr_eval)\n",
    "            if ssim_eval > max_ssim and psnr_eval > max_psnr :\n",
    "                max_ssim = max(max_ssim,ssim_eval)\n",
    "                max_psnr = max(max_psnr,psnr_eval)\n",
    "                torch.save({\n",
    "                            'step': step,\n",
    "                            'max_psnr': max_psnr,\n",
    "                            'max_ssim': max_ssim,\n",
    "                            'ssims': ssims,\n",
    "                            'psnrs': psnrs,\n",
    "                            'losses': losses,\n",
    "                            'model': net.state_dict()\n",
    "                }, model_dir)\n",
    "                print(f'\\n model saved at step : {step} | max_psnr: {max_psnr:.4f} | max_ssim: {max_ssim:.4f}')\n",
    "\n",
    "    np.save(f'./numpy_files/{model_name}_{steps}_losses.npy',losses)\n",
    "    np.save(f'./numpy_files/{model_name}_{steps}_ssims.npy',ssims)\n",
    "    np.save(f'./numpy_files/{model_name}_{steps}_psnrs.npy',psnrs)\n",
    "\n",
    "def test(net, loader_test, max_psnr, max_ssim, step):\n",
    "    net.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    ssims, psnrs = [], []\n",
    "    for i, (inputs, targets) in enumerate(loader_test):\n",
    "        inputs = inputs.to(device); targets = targets.to(device)\n",
    "        pred = net(inputs)\n",
    "        # # print(pred)\n",
    "        # tfs.ToPILImage()(torch.squeeze(targets.cpu())).save('111.png')\n",
    "        # vutils.save_image(targets.cpu(),'target.png')\n",
    "        # vutils.save_image(pred.cpu(),'pred.png')\n",
    "        ssim1 = ssim(pred, targets).item()\n",
    "        psnr1 = psnr(pred, targets)\n",
    "        ssims.append(ssim1)\n",
    "        psnrs.append(psnr1)\n",
    "        #if (psnr1>max_psnr or ssim1 > max_ssim) and s :\n",
    "#             ts=vutils.make_grid([torch.squeeze(inputs.cpu()),torch.squeeze(targets.cpu()),torch.squeeze(pred.clamp(0,1).cpu())])\n",
    "#             vutils.save_image(ts,f'samples/{model_name}/{step}_{psnr1:.4}_{ssim1:.4}.png')\n",
    "#             s=False\n",
    "    return np.mean(ssims) ,np.mean(psnrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f11c8830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T18:26:58.498255Z",
     "iopub.status.busy": "2024-01-15T18:26:58.497487Z",
     "iopub.status.idle": "2024-01-15T23:05:16.986195Z",
     "shell.execute_reply": "2024-01-15T23:05:16.985060Z"
    },
    "papermill": {
     "duration": 16700.077771,
     "end_time": "2024-01-15T23:05:18.568049",
     "exception": false,
     "start_time": "2024-01-15T18:26:58.490278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:01<00:00, 339MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from scratch *** \n",
      "train loss: 0.08983 | step: 500/20000 | lr: 0.0001000 | time_used: 6.7\n",
      "step: 500 | ssim: 0.7238 | psnr: 18.8582\n",
      "\n",
      " model saved at step : 500 | max_psnr: 18.8582 | max_ssim: 0.7238\n",
      "train loss: 0.08118 | step: 1000/20000 | lr: 0.0001000 | time_used: 13.8\n",
      "step: 1000 | ssim: 0.7828 | psnr: 19.1610\n",
      "\n",
      " model saved at step : 1000 | max_psnr: 19.1610 | max_ssim: 0.7828\n",
      "train loss: 0.06218 | step: 1500/20000 | lr: 0.0001000 | time_used: 20.9\n",
      "step: 1500 | ssim: 0.8104 | psnr: 20.9885\n",
      "\n",
      " model saved at step : 1500 | max_psnr: 20.9885 | max_ssim: 0.8104\n",
      "train loss: 0.09905 | step: 2000/20000 | lr: 0.0001000 | time_used: 27.8\n",
      "step: 2000 | ssim: 0.8215 | psnr: 21.5018\n",
      "\n",
      " model saved at step : 2000 | max_psnr: 21.5018 | max_ssim: 0.8215\n",
      "train loss: 0.08462 | step: 2500/20000 | lr: 0.0001000 | time_used: 34.8\n",
      "step: 2500 | ssim: 0.8252 | psnr: 21.5874\n",
      "\n",
      " model saved at step : 2500 | max_psnr: 21.5874 | max_ssim: 0.8252\n",
      "train loss: 0.06083 | step: 3000/20000 | lr: 0.0001000 | time_used: 41.7\n",
      "step: 3000 | ssim: 0.8313 | psnr: 22.1653\n",
      "\n",
      " model saved at step : 3000 | max_psnr: 22.1653 | max_ssim: 0.8313\n",
      "train loss: 0.05087 | step: 3500/20000 | lr: 0.0001000 | time_used: 48.6\n",
      "step: 3500 | ssim: 0.8281 | psnr: 21.5799\n",
      "train loss: 0.08024 | step: 4000/20000 | lr: 0.0001000 | time_used: 55.5\n",
      "step: 4000 | ssim: 0.8356 | psnr: 21.6004\n",
      "train loss: 0.03787 | step: 4500/20000 | lr: 0.0001000 | time_used: 62.4\n",
      "step: 4500 | ssim: 0.8406 | psnr: 23.3394\n",
      "\n",
      " model saved at step : 4500 | max_psnr: 23.3394 | max_ssim: 0.8406\n",
      "train loss: 0.05399 | step: 5000/20000 | lr: 0.0001000 | time_used: 69.4\n",
      "step: 5000 | ssim: 0.8311 | psnr: 22.3465\n",
      "train loss: 0.06198 | step: 5500/20000 | lr: 0.0001000 | time_used: 76.2\n",
      "step: 5500 | ssim: 0.8432 | psnr: 22.8291\n",
      "train loss: 0.04077 | step: 6000/20000 | lr: 0.0001000 | time_used: 83.2\n",
      "step: 6000 | ssim: 0.8358 | psnr: 22.6677\n",
      "train loss: 0.09947 | step: 6500/20000 | lr: 0.0001000 | time_used: 90.0\n",
      "step: 6500 | ssim: 0.8281 | psnr: 21.4365\n",
      "train loss: 0.08147 | step: 7000/20000 | lr: 0.0001000 | time_used: 97.0\n",
      "step: 7000 | ssim: 0.8386 | psnr: 22.2174\n",
      "train loss: 0.13631 | step: 7500/20000 | lr: 0.0001000 | time_used: 103.9\n",
      "step: 7500 | ssim: 0.8443 | psnr: 23.4536\n",
      "\n",
      " model saved at step : 7500 | max_psnr: 23.4536 | max_ssim: 0.8443\n",
      "train loss: 0.08841 | step: 8000/20000 | lr: 0.0001000 | time_used: 110.9\n",
      "step: 8000 | ssim: 0.8354 | psnr: 22.7987\n",
      "train loss: 0.09725 | step: 8500/20000 | lr: 0.0001000 | time_used: 117.7\n",
      "step: 8500 | ssim: 0.8554 | psnr: 24.3468\n",
      "\n",
      " model saved at step : 8500 | max_psnr: 24.3468 | max_ssim: 0.8554\n",
      "train loss: 0.08961 | step: 9000/20000 | lr: 0.0001000 | time_used: 124.6\n",
      "step: 9000 | ssim: 0.8534 | psnr: 23.9122\n",
      "train loss: 0.02973 | step: 9500/20000 | lr: 0.0001000 | time_used: 131.6\n",
      "step: 9500 | ssim: 0.8593 | psnr: 24.7556\n",
      "\n",
      " model saved at step : 9500 | max_psnr: 24.7556 | max_ssim: 0.8593\n",
      "train loss: 0.06283 | step: 10000/20000 | lr: 0.0001000 | time_used: 138.5\n",
      "step: 10000 | ssim: 0.8539 | psnr: 24.5621\n",
      "train loss: 0.09161 | step: 10500/20000 | lr: 0.0001000 | time_used: 145.4\n",
      "step: 10500 | ssim: 0.8542 | psnr: 23.3334\n",
      "train loss: 0.06977 | step: 11000/20000 | lr: 0.0001000 | time_used: 152.3\n",
      "step: 11000 | ssim: 0.8620 | psnr: 25.0552\n",
      "\n",
      " model saved at step : 11000 | max_psnr: 25.0552 | max_ssim: 0.8620\n",
      "train loss: 0.06207 | step: 11500/20000 | lr: 0.0001000 | time_used: 159.2\n",
      "step: 11500 | ssim: 0.8638 | psnr: 25.0130\n",
      "train loss: 0.10213 | step: 12000/20000 | lr: 0.0001000 | time_used: 166.3\n",
      "step: 12000 | ssim: 0.8618 | psnr: 24.9091\n",
      "train loss: 0.09234 | step: 12500/20000 | lr: 0.0001000 | time_used: 173.3\n",
      "step: 12500 | ssim: 0.8615 | psnr: 24.7298\n",
      "train loss: 0.02918 | step: 13000/20000 | lr: 0.0001000 | time_used: 180.2\n",
      "step: 13000 | ssim: 0.8674 | psnr: 25.3698\n",
      "\n",
      " model saved at step : 13000 | max_psnr: 25.3698 | max_ssim: 0.8674\n",
      "train loss: 0.04041 | step: 13500/20000 | lr: 0.0001000 | time_used: 187.2\n",
      "step: 13500 | ssim: 0.8671 | psnr: 25.0020\n",
      "train loss: 0.06784 | step: 14000/20000 | lr: 0.0001000 | time_used: 194.1\n",
      "step: 14000 | ssim: 0.8629 | psnr: 25.5404\n",
      "train loss: 0.03521 | step: 14500/20000 | lr: 0.0001000 | time_used: 201.2\n",
      "step: 14500 | ssim: 0.8656 | psnr: 25.6086\n",
      "train loss: 0.06040 | step: 15000/20000 | lr: 0.0001000 | time_used: 208.2\n",
      "step: 15000 | ssim: 0.8567 | psnr: 25.3931\n",
      "train loss: 0.08077 | step: 15500/20000 | lr: 0.0001000 | time_used: 215.1\n",
      "step: 15500 | ssim: 0.8648 | psnr: 24.9405\n",
      "train loss: 0.03393 | step: 16000/20000 | lr: 0.0001000 | time_used: 222.0\n",
      "step: 16000 | ssim: 0.8738 | psnr: 25.7374\n",
      "\n",
      " model saved at step : 16000 | max_psnr: 25.7374 | max_ssim: 0.8738\n",
      "train loss: 0.04161 | step: 16500/20000 | lr: 0.0001000 | time_used: 229.1\n",
      "step: 16500 | ssim: 0.8706 | psnr: 25.9189\n",
      "train loss: 0.03756 | step: 17000/20000 | lr: 0.0001000 | time_used: 236.1\n",
      "step: 17000 | ssim: 0.8741 | psnr: 25.8796\n",
      "\n",
      " model saved at step : 17000 | max_psnr: 25.8796 | max_ssim: 0.8741\n",
      "train loss: 0.05716 | step: 17500/20000 | lr: 0.0001000 | time_used: 243.1\n",
      "step: 17500 | ssim: 0.8711 | psnr: 26.3667\n",
      "train loss: 0.10873 | step: 18000/20000 | lr: 0.0001000 | time_used: 250.0\n",
      "step: 18000 | ssim: 0.8724 | psnr: 26.1099\n",
      "train loss: 0.05524 | step: 18500/20000 | lr: 0.0001000 | time_used: 257.0\n",
      "step: 18500 | ssim: 0.8741 | psnr: 26.2847\n",
      "train loss: 0.04284 | step: 19000/20000 | lr: 0.0001000 | time_used: 263.9\n",
      "step: 19000 | ssim: 0.8720 | psnr: 25.4449\n",
      "train loss: 0.06129 | step: 19500/20000 | lr: 0.0001000 | time_used: 270.9\n",
      "step: 19500 | ssim: 0.8744 | psnr: 26.5542\n",
      "\n",
      " model saved at step : 19500 | max_psnr: 26.5542 | max_ssim: 0.8744\n",
      "train loss: 0.05593 | step: 20000/20000 | lr: 0.0001000 | time_used: 277.9\n",
      "step: 20000 | ssim: 0.8665 | psnr: 25.1979\n",
      "CPU times: user 4h 11min 59s, sys: 31min 19s, total: 4h 43min 19s\n",
      "Wall time: 4h 38min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ckp = torch.load(model_dir, map_location=device)\n",
    "# net = FFA(gps=gps, blocks=blocks)\n",
    "# net = nn.DataParallel(net)\n",
    "# net.load_state_dict(ckp['model'])\n",
    "loader_train = loaders_[trainset]\n",
    "loader_test = loaders_[testset]\n",
    "net = models_[network]\n",
    "net = net.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "criterion = []\n",
    "criterion.append(nn.L1Loss().to(device))\n",
    "if perloss:\n",
    "    vgg_model = vgg16(pretrained=True).features[:16]\n",
    "    vgg_model = vgg_model.to(device)\n",
    "    for param in vgg_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    criterion.append(PerLoss(vgg_model).to(device))\n",
    "optimizer = optim.Adam(params = filter(lambda x: x.requires_grad, net.parameters()), lr=learning_rate, betas=(0.9,0.999), eps=1e-08)\n",
    "optimizer.zero_grad()\n",
    "train(net, loader_train, loader_test, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26012c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-15T23:05:21.867537Z",
     "iopub.status.busy": "2024-01-15T23:05:21.866799Z",
     "iopub.status.idle": "2024-01-15T23:05:28.085029Z",
     "shell.execute_reply": "2024-01-15T23:05:28.083628Z"
    },
    "papermill": {
     "duration": 7.834563,
     "end_time": "2024-01-15T23:05:28.086644",
     "exception": true,
     "start_time": "2024-01-15T23:05:20.252081",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_dir: pred_FFA_its/\n",
      "initialization done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/o-haze/hazyd/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m net\u001b[38;5;241m.\u001b[39mload_state_dict(ckp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     22\u001b[0m     haze \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_dir\u001b[38;5;241m+\u001b[39mim)\n\u001b[1;32m     23\u001b[0m     haze\u001b[38;5;241m=\u001b[39mhaze\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), Image\u001b[38;5;241m.\u001b[39mBICUBIC)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/o-haze/hazyd/'"
     ]
    }
   ],
   "source": [
    "# its or ots\n",
    "task = 'its'\n",
    "# test imgs folder\n",
    "test_imgs = '/kaggle/input/o-haze/hazyd/'\n",
    "\n",
    "dataset = task\n",
    "img_dir = test_imgs\n",
    "\n",
    "output_dir = f'pred_FFA_{dataset}/'\n",
    "print(\"pred_dir:\",output_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "ckp = torch.load(model_dir, map_location=device)\n",
    "net = FFA(gps=gps, blocks=blocks)\n",
    "net = nn.DataParallel(net)\n",
    "net.load_state_dict(ckp['model'])\n",
    "net.eval()\n",
    "\n",
    "for im in os.listdir(img_dir):\n",
    "    haze = Image.open(img_dir+im)\n",
    "    haze=haze.resize((256, 256), Image.BICUBIC)\n",
    "    haze1 = tfs.Compose([\n",
    "        tfs.ToTensor(),\n",
    "        tfs.Normalize(mean=[0.64, 0.6, 0.58],std=[0.14,0.15, 0.152])\n",
    "    ])(haze)[None,::]\n",
    "    haze_no = tfs.ToTensor()(haze)[None,::]\n",
    "    with torch.no_grad():\n",
    "        pred = net(haze1)\n",
    "    ts = torch.squeeze(pred.clamp(0,1).cpu())\n",
    "    # tensorShow([haze_no, pred.clamp(0,1).cpu()],['haze', 'pred'])\n",
    "    \n",
    "    haze_no = make_grid(haze_no, nrow=1, normalize=True)\n",
    "    ts = make_grid(ts, nrow=1, normalize=True)\n",
    "    image_grid = torch.cat((haze_no, ts), -1)\n",
    "    vutils.save_image(image_grid, output_dir+im.split('.')[0]+'_FFA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92e43a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7b84b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b0092",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc6108",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4217314,
     "sourceId": 7274509,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4308958,
     "sourceId": 7408658,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16739.195672,
   "end_time": "2024-01-15T23:05:31.632890",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-15T18:26:32.437218",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
